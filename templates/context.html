{% extends "base.html" %} {% block title %}Context - LLM Moral Judgment Study{%
endblock %} {% block header_title %}Context{% endblock %} {% block extra_head %}
<style>
  html {
    scroll-behavior: smooth;
  }
  section[id] {
    scroll-margin-top: 100px;
  }
</style>
{% endblock %} {% block content %}
<!-- MOTIVATION SECTION -->
<section id="motivation-section">
  <section class="main_intro centered">
    <aside>
      <h2>Why this research?</h2>
      <p>
        I want to understand how well a LLM, given a human-like personality, can
        reproduce human moral and values.
      </p>
    </aside>
    <article>
      <h2>Personal motivation</h2>
      <p>
        I've always liked personality tests and would be delighted to learn more
        about cognitive science. I want to see if a LLM, when given our
        personality, makes the same moral decisions than us.
      </p>
    </article>
  </section>

  <section class="page-content">
    <article class="content-block">
      <h2>Distinction from previous researches</h2>
      <p>
        With the emergence of Large Language Models (LLMS), many studies have
        already been conducted on their behavior when facing ethical dilemmas.
        However, the link between personality and moral values have not yet been
        used to evaluate a LLM behavior, thus making me ask myself: can a LLM,
        while reproducing our personality traits, replicate our moral behavior?
      </p>
    </article>

    <article class="content-block">
      <h2>Linked to several subjects</h2>
      <div class="impact-list">
        <div class="impact-item">
          <h3>Psychology Research</h3>
          <p>
            Understanding to what extend can a LLM accurately replicate one's
            personality, and which open-source model is the best at doing so.
            This could tell if AI agents could in the future be used as
            participants in psychology research.
          </p>
        </div>
        <div class="impact-item">
          <h3>AI Development</h3>
          <p>
            Giving us insights on how personality simulation affects moral
            reasoning in AI systems.
          </p>
        </div>
        <div class="impact-item">
          <h3>Ethics</h3>
          <p>
            In a world where everybody is stressed out by the rise of evil AI
            robots, it could help find ways to give AI moral values thus making
            it more ethical.
          </p>
        </div>
      </div>
    </article>
  </section>
</section>

<!-- OBJECTIVES SECTION -->
<section id="objectives-section">
  <section class="page-content">
    <article class="content-block">
      <h2>Research Objectives</h2>
      <p>
        This study aims to explore how artificial agents built on Large Language
        Models can replicate human moral judgment in complex ethical dilemmas.
      </p>
    </article>

    <div class="impact-list">
      <div class="impact-item">
        <h3>Objective 1</h3>
        <p>[Add your objective here]</p>
      </div>

      <div class="impact-item">
        <h3>Objective 2</h3>
        <p>[Add your objective here]</p>
      </div>

      <div class="impact-item">
        <h3>Objective 3</h3>
        <p>[Add your objective here]</p>
      </div>
    </div>

    <article class="content-block highlight">
      <h2>Expected Outcomes</h2>
      <p>[Add your expected outcomes here]</p>
    </article>
  </section>
</section>
{% endblock %}
