{% extends "base.html" %} {% block title %}About Study - LLM Moral Judgment
Study{% endblock %} {% block header_title %}About Study{% endblock %} {% block
extra_head %}
<style>
  html {
    scroll-behavior: smooth;
  }
  section[id] {
    scroll-margin-top: 100px;
  }

  /* Study Report Section - Minimalist Black & White */
  #study-report-section {
    background: #000000;
  }

  #study-report-section .page-content {
    max-width: 900px;
    margin: 0 auto;
    padding: 40px 20px;
  }

  #study-report-section h1 {
    color: #ffffff;
    font-size: 28px;
    font-weight: bold;
    text-align: center;
    margin-bottom: 40px;
    line-height: 1.4;
  }

  #study-report-section h2 {
    color: #ffffff;
    font-size: 20px;
    font-weight: bold;
    margin-top: 40px;
    margin-bottom: 15px;
  }

  #study-report-section p {
    color: #ffffff;
    font-size: 16px;
    line-height: 1.8;
    margin-bottom: 20px;
    text-align: justify;
  }

  #study-report-section code {
    background: #1a1a1a;
    padding: 2px 6px;
    border-radius: 3px;
    font-family: monospace;
  }

  #study-report-section em {
    font-style: italic;
  }

  /* Box plot image - Simple */
  .boxplot-container {
    text-align: center;
    margin: 40px auto;
  }

  .boxplot-container img {
    max-width: 100%;
    height: auto;
    border: 1px solid #333;
  }

  .figure-caption {
    color: #cccccc;
    font-size: 14px;
    margin-top: 10px;
    text-align: center;
  }

  /* T-test table - Minimalist Black & White */
  .ttest-section {
    margin: 50px auto;
    text-align: center;
  }

  .ttest-section h3 {
    color: #ffffff;
    font-size: 18px;
    font-weight: bold;
    margin-bottom: 20px;
  }

  .ttest-table {
    width: 100%;
    max-width: 600px;
    margin: 0 auto;
    border-collapse: collapse;
    background: #000000;
    border: 1px solid #ffffff;
  }

  .ttest-table th {
    background: #000000;
    color: #ffffff;
    padding: 12px;
    text-align: center;
    font-weight: bold;
    font-size: 14px;
    border: 1px solid #ffffff;
  }

  .ttest-table td {
    color: #ffffff;
    padding: 12px;
    font-size: 14px;
    text-align: center;
    border: 1px solid #ffffff;
  }

  /* Bibliography boxes */
  .bibliography-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
    gap: 25px;
    margin-top: 30px;
  }

  .reference-box {
    background: linear-gradient(
      135deg,
      rgba(27, 109, 128, 0.15),
      rgba(8, 107, 105, 0.15)
    );
    padding: 25px;
    border-radius: 12px;
    border-left: 4px solid #8c78c8;
    transition: transform 0.3s ease, box-shadow 0.3s ease;
  }

  .reference-box:hover {
    transform: translateY(-5px);
    box-shadow: 0 8px 20px rgba(140, 120, 200, 0.3);
  }

  .reference-authors {
    color: #8c78c8;
    font-weight: bold;
    font-size: 16px;
    margin-bottom: 8px;
  }

  .reference-title {
    color: white;
    font-size: 15px;
    font-style: italic;
    margin-bottom: 8px;
    line-height: 1.5;
  }

  .reference-details {
    color: #aaa;
    font-size: 13px;
    line-height: 1.6;
  }

  .reference-link {
    display: inline-block;
    margin-top: 12px;
    color: #f8d697;
    text-decoration: none;
    font-size: 13px;
    transition: color 0.3s ease;
  }

  .reference-link:hover {
    color: #ffd700;
    text-decoration: underline;
  }
</style>
{% endblock %} {% block content %}

<!-- STUDY REPORT SECTION -->
<section id="study-report-section">
  <!-- SUBPAGE TITLE -->
  <h1 class="subpage-title-white">Study Report</h1>

  <section class="page-content">
    <h2>Introduction</h2>
    <p>
      This study compares the moral decision-making of human participants and
      artificial agents (LLaMA-based) using the CNI model, which decomposes
      judgments into sensitivity to consequences (C), sensitivity to norms (N),
      and general preference for inaction versus action (I). The goal is to test
      whether artificial agents reproduce the typical human CNI pattern or
      exhibit a distinct moral profile.
    </p>

    <h2>Methods</h2>
    <p>
      Human data were drawn from previously collected CNI studies using
      standardized moral dilemmas and the original scoring procedure described
      on the CNI website, while artificial data were generated by prompting
      LLaMA agents with the same dilemmas and extracting C, N, and I from their
      choices. For each participant or agent, three CNI parameters (C, N, I)
      were estimated and stored in a joint dataset
      (<code>dataset_with_artificial_cni.csv</code>), along with Big Five
      personality scores for descriptive purposes. Independent-samples t-tests
      compared mean C, N, and I between humans and artificial agents, and
      distributions were visualized with histograms and boxplots to assess the
      overall profile of each group.
    </p>

    <h2>Results</h2>
    <p>
      On average, humans showed higher sensitivity to consequences (C) than
      artificial agents (humans ≈ 0.27, agents ≈ 0.16), but lower norm
      sensitivity (N; humans ≈ 0.73, agents ≈ 0.83) and a lower preference for
      inaction (I; humans ≈ 0.61, agents ≈ 0.74). In other words, human
      judgments leaned more toward weighing outcomes, whereas artificial agents
      prioritized rule-following and inaction.
    </p>

    <p>
      Independent-samples t-tests confirmed that these differences were
      statistically reliable. Humans were more consequence-sensitive than
      agents, <em>t</em>(df<sub>C</sub>) ≈ 14.6, <em>p</em> < .001; agents were
      more norm-sensitive than humans, <em>t</em>(df<sub>N</sub>) ≈ 4.4,
      <em>p</em> < .001; and agents also showed a stronger general preference
      for inaction, <em>t</em>(df<sub>I</sub>) ≈ 3.4, <em>p</em> < .001. Figure
      1 (human vs. artificial CNI boxplots) visualizes this pattern, with the
      human group showing a relatively more "consequentialist" profile and the
      artificial group a more "deontological and inaction-oriented" profile.
    </p>

    <!-- Box Plot Image -->
    <div class="boxplot-container">
      <img
        src="{{ url_for('static', filename='boxplot.png') }}"
        alt="Box plots comparing human vs artificial CNI parameters"
      />
      <p class="figure-caption">
        <strong>Figure 1.</strong> Distribution of CNI parameters (C, N, I) for
        human participants and artificial agents
      </p>
    </div>

    <!-- T-Test Results Table -->
    <div class="ttest-section">
      <h3>T-Test Results</h3>
      <table class="ttest-table">
        <thead>
          <tr>
            <th>Parameter</th>
            <th><em>t</em>-value</th>
            <th><em>p</em>-value</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>C</td>
            <td>14.64</td>
            <td>2.8e-38&lt; 0.001</td>
          </tr>
          <tr>
            <td>N</td>
            <td>4.42</td>
            <td>1.4e-05&lt; 0.001</td>
          </tr>
          <tr>
            <td>I</td>
            <td>3.38</td>
            <td>8.3e-5&lt; 0.001</td>
          </tr>
        </tbody>
      </table>
    </div>

    <h2>Discussion</h2>
    <p>
      The results indicate that LLaMA-based artificial agents do not simply
      replicate human CNI profiles but instead display systematically higher
      norm sensitivity and inaction preferences, coupled with reduced
      consequence sensitivity. This pattern suggests that current large language
      model agents tend to over-weight rule-compliance and avoidance of action
      compared to typical human moral judgments in the same dilemmas.
    </p>

    <p>
      From a theoretical perspective, the artificial agents' profile resembles
      an exaggerated deontological-inaction pattern, which may reflect how
      normative language and safety constraints are encoded during training.
      Practically, these findings imply that deploying such agents in real-world
      decision-support roles could result in decisions that are more
      conservative and norm-driven than those of human decision-makers facing
      comparable trade-offs between harms and rules.
    </p>
  </section>
</section>

<!-- ✅ SEPARATOR BETWEEN SUBPAGES -->
<div class="section-separator separator-results"></div>

<!-- BIBLIOGRAPHY SECTION -->
<section id="bibliography-section">
  <!-- SUBPAGE TITLE -->
  <h2 class="subpage-title-white">Bibliography</h2>

  <section class="page-content">
    <div class="bibliography-grid">
      <div class="reference-box">
        <div class="reference-authors">Luke, D. M., & Gawronski, B.</div>
        <div class="reference-title">
          Big Five Personality Traits and Moral-Dilemma Judgments: Two
          Preregistered Studies Using the CNI Model
        </div>
        <div class="reference-details">
          Journal of Personality and Social Psychology, 122(3), 454-478.
          <br />2022
        </div>
        <a
          href="https://www.researchgate.net/publication/363587918_Big_Five_Personality_Traits_and_Moral-Dilemma_Judgments_Two_Preregistered_Studies_Using_the_CNI_Model"
          class="reference-link"
          >View Article →</a
        >
      </div>

      <div class="reference-box">
        <div class="reference-authors">
          Chiu, T. K., Xia, Q., Zhou, X., Chai, C. S., & Cheng, M.
        </div>
        <div class="reference-title">
          Systematic literature review on opportunities, challenges, and future
          research recommendations of artificial intelligence in education
        </div>
        <div class="reference-details">
          Computers and Education: Artificial Intelligence, 4, 100118.
          <br />2024
        </div>
        <a
          href="https://www.sciencedirect.com/science/article/pii/S2666920X2200073X"
          class="reference-link"
          >View Article →</a
        >
      </div>

      <div class="reference-box">
        <div class="reference-authors">McCrae, R. R., & Costa, P. T.</div>
        <div class="reference-title">The Five-Factor Theory of Personality</div>
        <div class="reference-details">
          Handbook of Personality: Theory and Research, 3rd Edition. Guilford
          Press.
          <br />2008
        </div>
        <a
          href="https://psycnet.apa.org/record/2008-11667-005"
          class="reference-link"
          >View Article →</a
        >
      </div>

      <div class="reference-box">
        <div class="reference-authors">Touvron, H., et al.</div>
        <div class="reference-title">
          LLaMA: Open and Efficient Foundation Language Models
        </div>
        <div class="reference-details">
          arXiv preprint arXiv:2302.13971
          <br />2023
        </div>
        <a href="https://arxiv.org/pdf/2302.13971" class="reference-link"
          >View Article →</a
        >
      </div>

      <div class="reference-box">
        <div class="reference-authors">Kröner A, Gawronski B.</div>
        <div class="reference-title">
          Using the CNI model to test the automaticity of moral judgments
        </div>
        <div class="reference-details">
          Journal of Personality and Social Psychology, 122(3), 454-478.
          <br />2020
        </div>
        <a
          href="https://pubmed.ncbi.nlm.nih.gov/32111135/"
          class="reference-link"
          >View Article →</a
        >
      </div>

      <div class="reference-box">
        <div class="reference-authors">
          Gawronski, B., Armstrong, J., Conway, P., Friesdorf, R., & Hütter, M.
        </div>
        <div class="reference-title">
          Consequences, norms, and generalized inaction in moral dilemmas: The
          CNI model of moral decision-making
        </div>
        <div class="reference-details">
          Journal of Personality and Social Psychology, 113(3), 343-376.
          <br />2017
        </div>
        <a
          href="https://www.bertramgawronski.com/documents/GACFH2017JPSP.pdf"
          class="reference-link"
          >View Article →</a
        >
      </div>

      <div class="reference-box">
        <div class="reference-authors">
          Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S.
        </div>
        <div class="reference-title">
          On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?
        </div>
        <div class="reference-details">
          Proceedings of the 2021 ACM Conference on Fairness, Accountability,
          and Transparency (FAccT '21), 610-623.
          <br />2021
        </div>
        <a
          href="https://dl.acm.org/doi/epdf/10.1145/3442188.3445922"
          class="reference-link"
          >View Article →</a
        >
      </div>

      <div class="reference-box">
        <div class="reference-authors">Awad, E., et al.</div>
        <div class="reference-title">The Moral Machine experiment</div>
        <div class="reference-details">
          Nature, 563(7729), 59-64.
          <br />2018
        </div>
        <a href="https://www.moralmachine.net" class="reference-link"
          >View Article →</a
        >
      </div>

      <div class="reference-box">
        <div class="reference-authors">Dignum, V.</div>
        <div class="reference-title">
          Responsible Artificial Intelligence: How to Develop and Use AI in a
          Responsible Way
        </div>
        <div class="reference-details">
          Springer Nature.
          <br />2019
        </div>
        <a
          href="https://link.springer.com/book/10.1007/978-3-030-30371-6"
          class="reference-link"
          >View Article →</a
        >
      </div>
    </div>
  </section>
</section>

{% endblock %}
